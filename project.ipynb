{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_yuval_maya.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"GB7FsRk_q77R","colab_type":"text"},"cell_type":"markdown","source":["## Mount the drive\n"," please copy the code in the link that will apear and insert it in the box\n"]},{"metadata":{"id":"eI84fUyXk15E","colab_type":"code","outputId":"33984152-0f75-4284-bf83-1407e40f842b","executionInfo":{"status":"ok","timestamp":1547488177388,"user_tz":-120,"elapsed":39394,"user":{"displayName":"Aviv Nahon","photoUrl":"","userId":"11990176089380258211"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","os.chdir(\"drive/Team Drives/ml_hw3_yuval_maya\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"dCCxZ4J5r_XX","colab_type":"text"},"cell_type":"markdown","source":["## Import Libraries"]},{"metadata":{"id":"90FmtlmQl_3y","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","from sklearn import ensemble\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","from sklearn.model_selection import KFold\n","import time\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_recall_fscore_support as score, precision_score\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif, mutual_info_classif, chi2\n","import csv\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","import json\n","import pickle\n","import numpy as np\n","import pandas as pd\n","!pip install tensorpack"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nfhi6LuSsNE5","colab_type":"text"},"cell_type":"markdown","source":["## Define the Decorate class"]},{"metadata":{"id":"KGsZ_WyTmTh-","colab_type":"code","colab":{}},"cell_type":"code","source":["class Decorate(object):\n","  \n","    def __init__(self, tree_parameters, gan_parameters, synth_data, C_max_size=100, R_size=0.5, Itr_max=300, generator='basic'):\n","        \"\"\"\n","        Initializing the Decorate that will be in use to build the ensemble classifier\n","        :param tree_parameters: The parameters of the base learning algorithm ('Decision Tree Classifier')\n","        :param C_max_size: Maximum desired ensemble size\n","        :param R_size: Factor that determines number of artificial examples to generate\n","        :param Itr_max: Maximum number of iterations to build an ensemble\n","        :param generator: 'basic' VS 'GAN'\n","        \"\"\"\n","        self.tree_parameters = tree_parameters\n","        self.gan_parameters = gan_parameters\n","        self.C_max_size = C_max_size\n","        self.R_size = R_size\n","        self.Itr_max = Itr_max\n","        self.C_all = []\n","        self.classes = None\n","        self.generator = generator\n","        self.gan_synthetic_data = synth_data\n","        \n","\n","    def fit(self, X_train, Y_train, ds_name):\n","        \"\"\"\n","        Runs the DECORATE algorithm from the article (\"Creating Diversity In Ensembles Using Artificial Data\" p.8) \n","        with either the distribution based method for generating synthesised data or with GAN\n","        :param X_train: The training input samples\n","        :param Y_train: The target values\n","        \"\"\"\n","        C_i = DecisionTreeClassifier(**self.tree_parameters)\n","        self.C_all.append(C_i.fit(X_train, Y_train))\n","        C_size, I_number = 0, 0\n","        self.classes = pd.unique(Y_train)\n","        ensemble_error = self._compute_error(X_train, Y_train)           \n","        kf = KFold(n_splits=np.minimum(self.Itr_max, self.C_max_size)) \n","        \n","        # while C_size < self.C_max_size and I_number < self.Itr_max:\n","        for current_synt, _ in kf.split(self.gan_synthetic_data):\n","            if self.generator == 'basic':\n","                training_artificial_examples = self._generate_artificial_examples_basic(X_train)\n","            elif self.generator == 'GAN':\n","                training_artificial_examples = pd.DataFrame(self.gan_synthetic_data.iloc[current_synt].values, columns=list(X_train))\n","            artificial_labels_examples = self._generate_artificial_labels(training_artificial_examples)\n","            training_data = pd.concat([X_train, training_artificial_examples], axis=0).reset_index(drop=True)\n","            training_labels = np.concatenate((Y_train, artificial_labels_examples), axis=0)\n","            C_candidate = DecisionTreeClassifier(**self.tree_parameters)\n","            self.C_all.append(C_candidate.fit(training_data, training_labels))\n","            training_error = self._compute_error(X_train, Y_train)\n","            if training_error <= ensemble_error:\n","                ensemble_error = training_error\n","                C_size += 1\n","            else:\n","                self.C_all.pop()\n","            I_number = I_number + 1\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Predict class probabilities of the input samples X\n","        :param X: Input samples\n","        :return:\n","        \"\"\"\n","        probas = np.zeros((X.shape[0], len(self.classes)))\n","        for tree in self.C_all:\n","            probas += tree.predict_proba(X)\n","        return probas / len(self.C_all)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict class value for X\n","        :param X: Input samples\n","        :return:\n","        \"\"\"\n","        probas_list = self.predict_proba(X)\n","        results = np.argmax(probas_list, axis=1)\n","        return results\n","\n","    def _compute_error(self, X, Y):\n","        \"\"\"\n","        Compute ensemble error\n","        :param X: Input samples\n","        :param Y: Ground truth (correct) labels\n","        :return:\n","        \"\"\"\n","        pred = np.argmax(self.predict_proba(X), axis=1)\n","        ensemble_error = round(1 - accuracy_score(Y, pred), 5)\n","        return ensemble_error\n","\n","    def _generate_artificial_examples_basic(self, X):\n","        \"\"\"\n","        Generate artificial training data by randomly picking data points from an approximation of the training-data \n","        distribution.\n","        :param X: Input samples\n","        \"\"\"\n","        number_of_samples = int(X.shape[0] * self.R_size)\n","        artificial_examples = np.zeros((number_of_samples, X.shape[1]))\n","        for col_index, column_name in enumerate(X.columns):\n","            column = X[column_name]\n","            if len(pd.unique(column)) == 2 and (pd.unique(column)[0] == 0 or pd.unique(column)[0] == 1) and (\n","                    pd.unique(column)[1] == 0 or pd.unique(column)[1] == 1):\n","                proba_1 = column.tolist().count(1) / len(column)\n","                artificial_examples[:, col_index] = np.random.choice([0, 1], size=number_of_samples,\n","                                                                     p=[1 - proba_1, proba_1])\n","            else:\n","                artificial_examples[:, col_index] = np.random.normal(np.mean(column), np.std(column),\n","                                                                     size=number_of_samples)\n","        return pd.DataFrame(artificial_examples, columns=X.columns)\n","\n","    def _generate_artificial_labels(self, training_artificial_examples):\n","        \"\"\"\n","        first, giving to each training artificial example the class membership probabilities predicted by the ensemble.\n","        then, replace zero probabilities with 0.001 and normalize it. next, labels are selected, such that the \n","        probability of selection is inversely proportional to the current ensemble’s predictions.\n","        :param training_artificial_examples: Generated artificial training data\n","        \"\"\"\n","        labels = np.zeros(training_artificial_examples.shape[0])\n","        probabilities = self.predict_proba(training_artificial_examples)\n","        normalized_probabilities = self._normalize_probas(probabilities)\n","\n","        for prob in range(len(normalized_probabilities)):\n","            normalized_probabilities[prob] = 1 / normalized_probabilities[prob]\n","            normalized_probabilities[prob] = normalized_probabilities[prob] / np.sum(normalized_probabilities[prob])\n","            labels[prob] = np.random.choice(self.classes, p=normalized_probabilities[prob])\n","        return labels\n","\n","    def _normalize_probas(self, probabilities):\n","        \"\"\"\n","        Replace zero probabilities with 0.001 and normalize the probabilities to make it a distribution.\n","        :param probabilities: Probabilities predicted by the ensemble\n","        \"\"\"\n","        for probas in range(len(probabilities)):\n","            probabilities[probas] = [0.001 if p == 0.0 else p for p in probabilities[probas]] / np.sum(\n","                probabilities[probas])\n","        return probabilities"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nlc7fkW_ICyX","colab_type":"text"},"cell_type":"markdown","source":["## GAN synthetic data generation\n"]},{"metadata":{"id":"-iDiVlovIMEk","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_artificial_examples_GAN(X_train, Y_train, ds_name, GAN_parameters):\n","    \"\"\"\n","    Generates artificial training data with GAN algorithm\n","    :param X_train: The training input samples\n","    :param Y_train: The target values\n","    :param ds_name: The dataset name\n","    :param GAN_parameters: GAN algorithm parameters\n","    \"\"\"\n","    sample_rows = int(0.9*X_train.shape[0]*GAN_parameters['R_size']*np.minimum(GAN_parameters['Itr_max'], GAN_parameters['C_max_size']))\n","    cols = list(range(X_train.shape[1]))\n","    current_config = {\n","        \"name\": ds_name,\n","        \"num_random_search\": GAN_parameters['random_search'],\n","        \"train_csv\": \"data/\"+ds_name+\".csv\",\n","        \"continuous_cols\": cols,\n","        \"epoch\": GAN_parameters['epoch'],\n","        \"steps_per_epoch\": GAN_parameters['steps_per_epoch'],\n","        \"output_epoch\": GAN_parameters['output_epoch'],\n","        \"sample_rows\": sample_rows\n","    }\n","\n","    with open('current_config.json', 'w') as fp:\n","        json.dump([current_config], fp)\n","\n","    df = X_train.copy(deep=True)\n","    df['Class'] = Y_train\n","    df.to_csv('data/'+ds_name+\".csv\", header=False, index=False)\n","    !python3 src/launcher.py current_config.json\n","    exp_results = pd.read_csv('expdir/'+ds_name+'/exp-result.csv', header=None).values\n","    indices = np.where(exp_results == exp_results.max())\n","    synt_data = pd.read_csv('expdir/'+ds_name+'/synthetic'+str(indices[0][0])+'_'+str(GAN_parameters['epoch'])+'.csv', header=None)\n","\n","    return synt_data.iloc[:,:-1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IazVIU4TswXH","colab_type":"text"},"cell_type":"markdown","source":["## Define evaluation functions"]},{"metadata":{"id":"HZx2sH0Pob20","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate_i_fold(Y_test, pred, fit_time,prediction_time, total_scores_func, dataset, generator_name):\n","    \"\"\"\n","    Calculating the different scores of each cross validation run and save it to a scores dictionary. \n","    :param Y_test: Ground truth (correct) labels\n","    :param pred: Predicted labels, as returned by a classifier.\n","    :param fit_time: Amount of tme that the fit function took\n","    :param prediction_time: Amount of tme that the pred function took\n","    :param total_scores_func: saves all CV results\n","    :param dataset: data set name\n","    :param generator_name: 'basic' VS 'GAN'\n","    :return: total_scores_func (dictionary)\n","    \"\"\"\n","    configuration = 'c1'\n","    accuracy = round(accuracy_score(Y_test, pred), 3)\n","    precision = round(precision_score(Y_test, pred, average='macro'), 3)\n","    recall = round(recall_score(Y_test, pred, average='macro'), 3)\n","    f1 = round(f1_score(Y_test, pred, average='macro'), 3)\n","    total_scores_func[dataset][generator_name][configuration]['fit_time'].append(round(float(fit_time), 2)) \n","    total_scores_func[dataset][generator_name][configuration]['prediction_time'].append(round(float(prediction_time), 2)) \n","    total_scores_func[dataset][generator_name][configuration]['accuracy'].append(accuracy)\n","    total_scores_func[dataset][generator_name][configuration]['precision'].append(precision)\n","    total_scores_func[dataset][generator_name][configuration]['recall'].append(recall)\n","    total_scores_func[dataset][generator_name][configuration]['f1'].append(f1)\n","    return total_scores_func\n","\n","def write_data_set_results_to_csv(dataset,total_scores,generators,total_rows):\n","    \"\"\"\n","    Writes the CV results to a csv file.\n","    :param dataset: data set name\n","    :param total_scores: saves all CV results\n","    :param generators: 'basic' VS 'GAN'\n","    \"\"\"\n","    configuration='c1'\n","    for generator_name in generators:\n","        log_list_test = [dataset,total_rows,generator_name,\n","                         np.mean(total_scores[dataset][generator_name][configuration]['fit_time']),\n","                         np.mean(total_scores[dataset][generator_name][configuration]['prediction_time']),\n","                         np.mean(total_scores[dataset][generator_name][configuration]['accuracy']),\n","                         np.mean(total_scores[dataset][generator_name][configuration]['precision']),\n","                         np.mean(total_scores[dataset][generator_name][configuration]['recall']),\n","                         np.mean(total_scores[dataset][generator_name][configuration]['f1']) ]\n","        writer = csv.writer(open(\"results.csv\", \"a\"), lineterminator='\\n', dialect='excel')\n","        writer.writerow(log_list_test)\n","\n","def write_headline():\n","    \"\"\"\n","    Writes the file header.\n","    \"\"\"\n","    log_list_header = ['dataset', 'total_rows','generator',  'fit_time','prediction_time', 'accuracy', 'precision', 'recall', 'f1']\n","    writer = csv.writer(open(\"results.csv\", \"a\"), lineterminator='\\n', dialect='excel')\n","    writer.writerow(log_list_header)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ik7xWycBual6","colab_type":"text"},"cell_type":"markdown","source":["## Datasets and hyperparameters"]},{"metadata":{"id":"R8d28R7Doo3L","colab_type":"code","colab":{}},"cell_type":"code","source":["data_sets = { \n","               'Accident_Casualties':'datasets/Accident_Casualties.csv',\n","               'bureau':'datasets/bureau.csv',\n","               'home_credit':'datasets/home_credit.csv',\n","               'isolet':'datasets/isolet.csv',\n","               'LendingClub Issued Loans':'datasets/LendingClub Issued Loans.csv', \n","               'magic04': 'datasets/magic04.csv',\n","               'Skin_NonSkin':'datasets/Skin_NonSkin.csv',\n","               'The_broken_machine':'datasets/The_broken_machine.csv',\n","              }\n","\n","generators = ['GAN', 'basic']\n","\n","configuration='c1'\n","\n","tree_parameters={'max_depth': 12, 'min_samples_leaf': 10 , 'class_weight': 'balanced'}\n","\n","GAN_parameters={'random_search': 4, 'epoch': 2, 'steps_per_epoch': 100, 'output_epoch': 1, 'C_max_size': 50, 'R_size': 0.3, 'Itr_max': 50}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nVy7VtNquvaC","colab_type":"text"},"cell_type":"markdown","source":["## Main"]},{"metadata":{"id":"oyj50CQquzQh","colab_type":"code","colab":{}},"cell_type":"code","source":["# write_headline()\n","total_scores={}\n","\n","for dataset in data_sets:\n","    # Reading and processing the data\n","    ds = pd.read_csv(data_sets[dataset])\n","    print(\"Dataset name: {}\".format(dataset))\n","    ds = ds.dropna().reset_index(drop=True)\n","    ds = shuffle(ds, random_state=10).reset_index(drop=True)\n","    total_rows = ds.shape[0]\n","    if total_rows > 50000:\n","      ds = ds[:50000]\n","    le = LabelEncoder()\n","    Y = ds.pop('class')\n","    X = pd.get_dummies(ds)\n","    Y =le.fit_transform(Y)\n","    Y=pd.DataFrame(data=Y, columns=['class'])\n","    X, Y = shuffle(X, Y, random_state=10)\n","    X = X.reset_index(drop=True)\n","    Y = Y.reset_index(drop=True)\n","    col_x = list(X)\n","    if X.shape[1]>13:\n","      X = SelectKBest(chi2, k=13).fit_transform(X, Y.values.ravel())\n","    X = pd.DataFrame(X, columns=col_x[:13])\n","    kf = KFold(n_splits=10)\n","    kf.get_n_splits(X)\n","    total_scores.setdefault(dataset, {})\n","    \n","    for generator_name in generators:\n","        Scores = {\n","        'c1': {'fit_time': [],'prediction_time': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","        }\n","        total_scores[dataset].setdefault(generator_name, Scores)\n","        \n","        \n","    # Generate synthesized data with GAN\n","    print('Generating synthetic data with GAN')\n","    gan_start_time = time.time()\n","    gan_synth_data = generate_artificial_examples_GAN(X, Y, dataset, GAN_parameters)\n","    gan_time = (time.time() - gan_start_time)\n","    print('Fininshed GAN')\n","    \n","    # Run the CV\n","    fold = 0\n","    for train_index, test_index in kf.split(X):\n","        for generator_name in generators:\n","            print('Fold '+ str(fold)+' Generator ' + generator_name)\n","            X_train=X.iloc[train_index]\n","            X_test=X.iloc[test_index]\n","            Y_train=Y.iloc[train_index]\n","            Y_test=Y.iloc[test_index] \n","            clf=Decorate(tree_parameters, GAN_parameters, gan_synth_data, C_max_size=50, R_size=0.3, Itr_max=50,generator=generator_name)\n","            start_time = time.time()\n","            clf.fit(X_train,Y_train['class'].values.tolist(), ds_name=str(dataset)+str(fold))\n","            fit_time = (time.time() - start_time)\n","            if generator_name == 'GAN':\n","              fit_time += gan_time/10\n","            start_time = time.time()\n","            pred = clf.predict(X_test)\n","            prediction_time = (time.time() - start_time)\n","            seconds = (time.time() - start_time)\n","            total_scores = evaluate_i_fold(Y_test, pred, fit_time,prediction_time, total_scores, dataset, generator_name)\n","            \n","        fold += 1\n","    write_data_set_results_to_csv(dataset, total_scores, generators,total_rows)\n"],"execution_count":0,"outputs":[]}]}